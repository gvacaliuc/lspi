{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car\n",
    "\n",
    "My work with LSPI + Mountain Car environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 10, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from lspi.util import generate_sample_data\n",
    "\n",
    "from lspi.policy import RandomPolicy, DiscreteActionBasisPolicy\n",
    "from lspi.basis import IndActionPolyStateBasis, IndActionRBFStateBasis, QuadraticBasis\n",
    "\n",
    "from lspi.optim import LSPI\n",
    "from lspi.optim import LSTDQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### close all env windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gym.core.env_closer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lspi.util import DiscreteEnvWrapper, random_discretization, linear_discretization\n",
    "\n",
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lspi.util import SampleGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefs = [\n",
    "    np.eye(3),\n",
    "    np.ones((3, 3)),\n",
    "    np.array([[1,1,-1],[1,-1,1],[-1,1,1]])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_rollouts = 1000\n",
    "num_steps = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = SampleGenerator(env, RandomPolicy(env)).sample(num_rollouts, num_steps)\n",
    "basis = QuadraticBasis(coefs, env.action_space, env.observation_space)\n",
    "lstdq = LSTDQ(basis, discount=0.95)\n",
    "dabp = DiscreteActionBasisPolicy(env.action_space, basis, np.zeros(basis.rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lspi = LSPI(lstdq, dabp, max_iter=50, epsilon=1e-3)\n",
    "lspi.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lspi.itr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "actions = []\n",
    "\n",
    "for episode in range(100):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cumr = 0\n",
    "    while not done:\n",
    "        action = lspi.policy(state)\n",
    "        actions.append(action)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        cumr += reward\n",
    "        env.render()\n",
    "    rewards.append(cumr)\n",
    "#         print(\"Action: {}, Reward: {}\".format(action, reward))\n",
    "#     print(\"Episode {episode}, Reward: {reward}\".format(episode=episode, reward=reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(cumr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBF Basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\")\n",
    "env._max_episode_steps = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_rollouts = 600\n",
    "num_steps = 20\n",
    "data = SampleGenerator(env, RandomPolicy(env)).sample(num_rollouts, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lspi.util import BoxAnchorGen\n",
    "\n",
    "numAnchors = 40\n",
    "anchors = BoxAnchorGen(numAnchors, env.observation_space)\n",
    "basis = IndActionRBFStateBasis(anchors, env.action_space, env.observation_space)\n",
    "dabp = DiscreteActionBasisPolicy(env.action_space, basis, np.zeros(basis.rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstdq = LSTDQ(basis, discount=0.95)\n",
    "lspi = LSPI(lstdq, dabp, max_iter=25, epsilon=1e-3)\n",
    "lspi.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lspi.itr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rewards = []\n",
    "actions = []\n",
    "\n",
    "for episode in range(50):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cumr = 0\n",
    "    while not done:\n",
    "        action = lspi.policy(state)\n",
    "        actions.append(action)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "#         done = cumr < -300 or state[0] > 0.5\n",
    "        cumr += reward\n",
    "#         env.render()\n",
    "#     print(cumr)\n",
    "#         print(\"Action: {}, State: {}, Reward: {}\".format(action, state, reward))\n",
    "    rewards.append(cumr)\n",
    "#     print(\"Episode {episode}, Reward: {reward}\".format(episode=episode, reward=reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.hist(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-04-04 03:40:02,121] Making new env: MountainCarContinuous-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([-1.]),\n",
       " array([-0.6]),\n",
       " array([-0.2]),\n",
       " array([ 0.2]),\n",
       " array([ 0.6]),\n",
       " array([ 1.])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lspi.util import DiscreteEnvWrapper, random_discretization, linear_discretization\n",
    "\n",
    "env = gym.make(\"MountainCarContinuous-v0\")\n",
    "actions = linear_discretization(env.action_space, n=6)\n",
    "dEnv = DiscreteEnvWrapper(env, actions)\n",
    "dEnv._actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lspi.util import SampleGenerator\n",
    "data = SampleGenerator(dEnv, RandomPolicy(dEnv)).sample(10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lspi.basis import DiscreteQuadraticBasis\n",
    "\n",
    "coefs = [\n",
    "    np.eye(3),\n",
    "    np.ones((3, 3)),\n",
    "    np.array([[1,1,-1],[1,-1,1],[-1,1,1]])\n",
    "]\n",
    "basis = DiscreteQuadraticBasis(coefs, dEnv._actions, dEnv.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  1.92783146,  3.73447137,  2.00178615])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basis(dEnv.observation_space.sample(), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lstdq = LSTDQ(basis, discount=0.95)\n",
    "dabp = DiscreteActionBasisPolicy(dEnv.action_space, basis, np.zeros(basis.rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lspi.optim.LSPI at 0x7fc6662bd7d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lspi = LSPI(lstdq, dabp, max_iter=50, epsilon=1e-3)\n",
    "lspi.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lspi.itr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "actions = []\n",
    "\n",
    "for episode in range(50):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    cumr = 0\n",
    "    while not done:\n",
    "        action = lspi.policy(state)\n",
    "        action = dEnv._actions[action]\n",
    "        actions.append(action)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "#         done = cumr < -300 or state[0] > 0.5\n",
    "        cumr += reward\n",
    "#         env.render()\n",
    "#     print(cumr)\n",
    "#         print(\"Action: {}, State: {}, Reward: {}\".format(action, state, reward))\n",
    "    rewards.append(cumr)\n",
    "#     print(\"Episode {episode}, Reward: {reward}\".format(episode=episode, reward=reward))\n",
    "rewards = np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   1.,   1.,   3.,   4.,  13.,   0.,   0.,   1.,   5.,   1.,\n",
       "         10.,   4.,   0.,   0.,   0.,   0.,   0.,   0.,   4.]),\n",
       " array([ 98.364 ,  98.3874,  98.4108,  98.4342,  98.4576,  98.481 ,\n",
       "         98.5044,  98.5278,  98.5512,  98.5746,  98.598 ,  98.6214,\n",
       "         98.6448,  98.6682,  98.6916,  98.715 ,  98.7384,  98.7618,\n",
       "         98.7852,  98.8086,  98.832 ]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAFpCAYAAAC4SK2+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAbEFADAwP8BVx7/0MPmKAMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc65ac3bb90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(rewards[rewards > 50], bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rigorous Testing\n",
    "\n",
    "OK.  So now we know we can actually solve this problem, and we'd like to\n",
    "explore when this algorithm works and when it doesn't.  To figure out the\n",
    "best way to go about this this testing, let's list what \"knobs\" we have\n",
    "and what \"dials\" we're looking at:\n",
    "\n",
    "Knobs:  \n",
    "* rbf basis: anchor points + how many\n",
    "* poly basis: order\n",
    "* discount\n",
    "* environment\n",
    "* number of testing episodes\n",
    "* data\n",
    "    * quantity\n",
    "    * distribution (random vs. educated policy)\n",
    "\n",
    "Dials:  \n",
    "* total reward\n",
    "* total steps\n",
    "* action distribution (since we're working in discrete this is p easy, could estimate some parametric dist. in continuous)\n",
    "\n",
    "\n",
    "## initial thoughts\n",
    "\n",
    "* we can sample from our observation space for new anchor points\n",
    "    * however, since the env. puts a $(-\\infty, \\infty)$ bound on velocities this will result in poor anchors\n",
    "    * I can get the bounds from a space and just \"clip\" them to reasonable amounts and create a new space\n",
    "* if we want to examine rbf + poly, we need a composite basis.  just going to use RBF for now\n",
    "* w/ reward + steps we can get avg. reward\n",
    "* Id like a dictionary for each testing event, as then I can do post-processing in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this generator in place of previous anchor points.  Now lets\n",
    "develop a framework for providing our inputs.  Not sure if some sort of \n",
    "object or simply a function makes more sense here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lspi.policy import Policy\n",
    "\n",
    "def run_test(policy, env, num_episodes = 100):\n",
    "    numsteps = []\n",
    "    action_ratio = []\n",
    "    rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        steps = 0\n",
    "        total_reward = 0\n",
    "        actions = []\n",
    "        while not done:\n",
    "            action = policy(state)\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            actions.append(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "        rewards.append(total_reward)\n",
    "        numsteps.append(steps)\n",
    "        action_ratio.append(np.mean(actions))\n",
    "        \n",
    "    return {\"steps\": numsteps, \"rewards\": rewards, \"action_ratio\": action_ratio}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = run_test(lspi.policy, env, num_episodes=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
